{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwxJqHJiiYHX"
      },
      "source": [
        "### Download and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_7D67c3Sc7Aa"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/xenova/transformers.js/main/scripts/requirements.txt\n",
        "!wget https://raw.githubusercontent.com/xenova/transformers.js/main/scripts/convert.py\n",
        "!pip install -r requirements.txt -q\n",
        "!pip install huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMNQfFlhc7Ac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi, notebook_login, whoami, create_repo, ModelCard, ModelCardData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELgIog2Zc7Ac"
      },
      "source": [
        "### Login to ðŸ¤— Hub\n",
        "Make sure you add HF_TOKEN to your notebook secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "foOxhxatc7Ad"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n",
        "user = whoami()['name']\n",
        "api = HfApi()\n",
        "print(f\"Logged in to ðŸ¤— as {user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETNvzRPac7Ad"
      },
      "source": [
        "### Setup Model Credentials\n",
        "Pick a model from the  [ðŸ¤— Hub](https://huggingface.co/models).  \n",
        "Set your username on the ðŸ¤— Hub to deploy the converted model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlM1874Fc7Ae"
      },
      "outputs": [],
      "source": [
        "model = \"textattack/bert-base-uncased-rotten-tomatoes\" # @param {type:\"string\"}\n",
        "task = \"text-classification\" # @param {type:\"string\"}\n",
        "user = \"codewithkyrian\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeNbjAVc7Ae"
      },
      "source": [
        "### Convert the model to ONNX\n",
        "This might take a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AZgFlpvdc7Ae"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"models\", exist_ok=True)\n",
        "!python convert.py --quantize --model_id $model --task $task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6Hna1xikyk"
      },
      "source": [
        "### Upload Converted model to ðŸ¤— Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TWHZV29c7Ae"
      },
      "outputs": [],
      "source": [
        "model_name = model.split(\"/\")[-1]\n",
        "repo_id = f\"{user}/{model_name}\" # new repo id with user\n",
        "print(f\"Repo ID: {repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57rgvUM0c7Ae"
      },
      "outputs": [],
      "source": [
        "# Create the repo\n",
        "repo_url = create_repo(repo_id, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acsuCAdwc7Af"
      },
      "outputs": [],
      "source": [
        "# upload the model to the hub\n",
        "api.upload_folder(\n",
        "    folder_path=f\"models/{model}\", # default output path from convert.py\n",
        "    repo_id=repo_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_3MYEYMc7Af"
      },
      "outputs": [],
      "source": [
        "card = ModelCard.load(model) # get the old model card\n",
        "card_meta = card.data.to_dict()\n",
        "card_meta['library_name'] = \"Transformers PHP\"\n",
        "card_meta.setdefault('tags', []) # sometimes, tags key doesn't exist\n",
        "card_meta['tags'] += [\"onnx\"]\n",
        "card_meta['pipeline_tag'] = task\n",
        "card_meta = ModelCardData(**card_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OFZzsogc7Af"
      },
      "outputs": [],
      "source": [
        "# Add transformers.js modelcard template to existing model card\n",
        "content = f\"\"\"\n",
        "---\n",
        "{card_meta.to_yaml()}\n",
        "---\n",
        "\n",
        "https://huggingface.co/{model} with ONNX weights to be compatible with Transformers PHP\n",
        "\n",
        "{card.text}\n",
        "---\n",
        "\n",
        "Note: Having a separate repo for ONNX weights is intended to be a temporary solution until ONNXRuntime gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [ðŸ¤— Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t364cswtc7Af"
      },
      "outputs": [],
      "source": [
        "ModelCard(content).push_to_hub(repo_id) # push the new model card to the hub"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
