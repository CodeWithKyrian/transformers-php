---
outline: deep
---

# Pipelines

Pipelines are a core feature of TransformersPHP, designed to simplify the use of machine learning models for various
natural language processing (NLP) tasks. They encapsulate the entire process of running a model, from input
preprocessing to post-processing the output, making it easy to integrate advanced NLP capabilities into
your applications.

## Creating a Pipeline

To create a pipeline, you use the pipeline function, specifying the task you want to perform. Here's a basic example for
sentiment analysis:

```php
use function Codewithkyrian\Transformers\Pipelines\pipeline;

$classifier = pipeline('sentiment-analysis');
```

This initializes a pipeline for sentiment analysis, automatically handling model downloading, caching,
input processing, and output interpretation.

## Pipeline with Options

Besides passing the task you want to perform, you can also customize the pipeline instance creation with some additional
options. For instance, you can specify different model to use instead of the default model:

```php
$classifier = pipeline('sentiment-analysis', 'nlptown/bert-base-multilingual-uncased-sentiment');
```

Beyond the task and model name, you can further tailor your pipeline with additional named arguments. Here's a breakdown
of these options for better clarity:

### `task`

Specifies the task you wish the pipeline to execute. Refer to
the [list of supported tasks](/pipelines#supported-tasks) for available options.

### `modelName`

Specifies the model to be used by the pipeline. You can use any ONNX model from the Hugging Face model repository that
is compatible with the specified task. You can also use your custom models, provided you've prepared them as instructed.
If not provided, the default model for the task will be used. Eg

```php
$generator = pipeline('text-generation', 'Xenova/codegen-350M-mono');
```

### `quantized`

A boolean value indicating whether to use a quantized version of the model. Quantization reduces the model size and
speeds up inference but may slightly decrease accuracy. This option defaults to false.

### `config`

Allows you to pass a custom configuration for the pipeline. This could include specific model parameters or
preprocessing options. Providing a custom config can help tailor the pipeline's behavior to better fit your application'
s requirements.

### `cacheDir`

While it's typically recommended to [set the cache directory globally](/configuration#setcachedir-string-cachedir),
this allows you to modify the cache directory to save and look for models for this pipelie instance.

### `revision`

This specified model version to use. It can be a branch name, a tag name, or a commit id. Since HuggingFace uses a
git-based system for storing models and other artifacts, so ``revision`` can be any identifier allowed by git.

### `modelFilename`

This specified the filename of the model in the repository. It's particularly used for decoder only models. It defaults
to `decoder_model_merged` but you can set it to use another if the repository doesn't use that nomenclature.

## Running a Pipeline

Once you've created a pipeline, running it is straightforward. All pipelines are designed to accept input text as their
primary argument. Here's how to run a pipeline for some common NLP tasks.

### Basic Usage

For tasks like sentiment analysis, text generation, or named entity recognition (NER), you typically provide a string or
an array of strings as input. Here's an example using the sentiment analysis pipeline created earlier:

```php
$result = $classifier("TransformersPHP makes NLP easy and accessible.");
```

### Handling Multiple Inputs

Most pipelines can also process multiple inputs in a single call, which is especially useful for batch processing.
Provide
an array of strings to analyze multiple texts at once:

```php
$results = $classifier([
    "I love using TransformersPHP for my projects.",
    "The weather today is dreadful."
]);
```

### Additional Options

Additional arguments can be passed to the pipeline function to customize it's behavior, but they are hugely dependent on
the task you're using the pipelines for. For example, for translation, you can specify the source and target languages:

```php
$translator = pipeline('translation', 'Xenova/m2m100_418M');

$result = $translator('I love TransformersPHP!', srcLang: 'en', tgtLang: 'fr');
```

Details on the specific options available for each pipeline task are provided within the documentation for that task.

### Pipeline Output

The output generated by a pipeline varies based on the task it's performing and the nature of the input provided. For
example:

For the classifier with one input, the output can be:

```php
['label' => 'POSITIVE',  'score' => 0.9995358059835]
```

and for the multiple input classifier:

```php
[
    ['label' => 'POSITIVE',  'score' => 0.99980061678407],
    ['label' => 'NEGATIVE',  'score' => 0.99842234422764],
]
```

and for the translation task:

```php
['translation_text' => 'J\'aime TransformersPHP!']
```

## Supported Tasks

#### Natural Language Processing

| Task                                                                              | ID                                            | Description                                                                                    | Supported? |
|-----------------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------------------------------|------------|
| [Fill-Mask](/fill-mask)                                                           | `fill-mask`                                   | Masking some of the words in a sentence and predicting which words should replace those masks. | ✅          |
| [Question Answering](/question-answering)                                         | `question-answering`                          | Retrieve the answer to a question from a given text.                                           | ✅          |
| [Sentence Similarity](/feature-extraction)                                        | `sentence-similarity`                         | Determining how similar two texts are.                                                         | ✅          |
| [Summarization](/summarization)                                                   | `summarization`                               | Producing a shorter version of a document while preserving its important information.          | ✅          |
| [Table Question Answering](https://huggingface.co/tasks/table-question-answering) | `table-question-answering`                    | Answering a question about information from a given table.                                     | ❌          |
| [Text Classification](/text-classification)                                       | `text-classification` or `sentiment-analysis` | Assigning a label or class to a given text.                                                    | ✅          |
| [Text Generation](/text-generation)                                               | `text-generation`                             | Producing new text by predicting the next word in a sequence.                                  | ✅          |
| [Text-to-text Generation](/text-to-text-generation)                               | `text2text-generation`                        | Converting one text sequence into another text sequence.                                       | ✅          |
| [Token Classification](/token-classification)                                     | `token-classification` or `ner`               | Assigning a label to each token in a text.                                                     | ✅          |
| [Translation](/translation)                                                       | `translation`                                 | Converting text from one language to another.                                                  | ✅          |
| [Zero-Shot Classification](/zero-shot-classification)                             | `zero-shot-classification`                    | Classifying text into classes that are unseen during training.                                 | ✅          |

## Supported Model Architectures

TransformersPHP supports a wide range of model architectures for various NLP tasks. If the specific model you're
interested in isn't listed here, you can open an issue on the repository so we can add support for it. Here's a list of
currently tested and supported model architectures:

1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota
   Technological Institute at Chicago) released with the
   paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942),
   by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the
   paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
   by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke
   Zettlemoyer.
1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the
   paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
   by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google)
   released with the
   paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha
   Rothe, Shashi Narayan, Aliaksei Severyn.
1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the
   paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by
   Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.
1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research)
   released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil
   Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula,
   Qifan Wang, Li Yang, Amr Ahmed.
1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released
   with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru
   Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
   Li Yang, Amr Ahmed.
1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the
   paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang,
   Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.
1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the
   paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang
   Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.
1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the
   paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng
   He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.
1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the
   paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng
   He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.
1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together
   with the
   paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
   by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2
   into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation),
   RoBERTa
   into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation),
   Multilingual BERT
   into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and
   a German version of DistilBERT.
1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University)
   released with the
   paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
   by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.
1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the
   repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
   by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa
   Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
   Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov,
   Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei
1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the
   paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec
   Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the
   repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and
   Aran Komatsuzaki.
1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the
   paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li,
   Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu,
   Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey
   Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu,
   Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David
   Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel
   Fried, Arjun Guha, Harm de Vries, Leandro von Werra.
1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the
   paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan,
   Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume
   Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli,
   Armand Joulin.
1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with
   the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
   by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the
   paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle
   Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook)
   released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038)
   by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.
1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the
   paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
   by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi
   Zhou and Wei Li and Peter J. Liu.
1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the
   repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)
   by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi
   Zhou and Wei Li and Peter J. Liu.